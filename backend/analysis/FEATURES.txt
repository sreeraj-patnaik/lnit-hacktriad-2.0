App: analysis
Project: Context-Aware AI Health Trajectory Interpreter
========================================================

PURPOSE
-------
AI-powered analysis engine. Processes medical reports to extract
insights, detect health trends, and generate human-readable summaries.

FEATURES TO IMPLEMENT
----------------------

1. TRIGGER ANALYSIS
   - POST /api/analysis/run/           → Run full analysis on a report
     Input:  { "report_id": <id> }
     Output: { "status": "success", "analysis_id": <id> }

2. ANALYSIS RESULT
   - GET /api/analysis/<id>/           → Retrieve result of a specific analysis
   - Fields returned: summary, key_findings, recommendations, confidence_score

3. HEALTH TRAJECTORY
   - GET /api/analysis/trajectory/<patient_id>/
     → Compare multiple reports over time to show health trend
     → Output: list of data points (date, metric, value, trend direction)

4. KEY METRIC EXTRACTION
   - Extract structured values from lab reports:
     e.g., HbA1c, Blood Pressure, Cholesterol, Hemoglobin, Creatinine
   - Store as HealthMetric records linked to reports

5. AI EXPLANATION GENERATION
   - POST /api/analysis/explain/
     Input:  { "report_id": <id>, "language": "simple" }
     Output: { "explanation": "plain English summary" }
   - Integrates with LLM (Gemini / OpenAI)

6. CONTEXT-AWARE INTERPRETATION
   - Consider patient history (past reports) when generating analysis
   - Flag if a value has worsened compared to previous reports

7. MODELS
   - AnalysisResult
     Fields: report (FK → Report), summary, key_findings (JSON),
             recommendations (JSON), confidence_score, created_at
   - HealthMetric
     Fields: patient, report, metric_name, value, unit, recorded_at, trend

DEPENDENCIES
------------
- google-generativeai  (Gemini API)  OR  openai
- PyPDF2 / pdfplumber  (PDF text extraction)
- Pillow               (image preprocessing for OCR)

NOTES
------
- Run analysis asynchronously using Celery + Redis for large files
- Cache repeated analysis results to reduce API costs
- Store raw LLM responses for audit/debug purposes
